"use strict";(self.webpackChunkplugnmeet_documentation=self.webpackChunkplugnmeet_documentation||[]).push([[1336],{8631:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});var i=t(43502),s=t(62615),a=t(31894);const r={title:"How to Add an Interactive AI Assistant to Your Video App in 15 Minutes",slug:"how-to-add-ai-meeting-assistant-features",authors:["jibon"],tags:["tutorial","how-to","ai-meeting-assistant","ai-text-chat","ai-meeting-notes","meeting-summary-ai","ai-meeting-summarizer","meeting-assistant-ai","developer"]},o=void 0,l={authorsImageUrls:[void 0]},c=[{value:"The Goal",id:"the-goal",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Configure the AI Providers",id:"step-1-configure-the-ai-providers",level:3},{value:"Step 2: Enable AI Features in Your Room",id:"step-2-enable-ai-features-in-your-room",level:3},{value:"Step 3: Experience It Live",id:"step-3-experience-it-live",level:3},{value:"Conclusion",id:"conclusion",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:["In today's competitive landscape, a basic video call is no longer enough. Users expect intelligent, accessible, and productive experiences. What if you could offer an ",(0,s.jsx)(n.strong,{children:"AI meeting assistant"})," that provides live translation, generates ",(0,s.jsx)(n.strong,{children:"AI meeting notes"}),", and can even answer questions directly in the chat?"]}),"\n",(0,s.jsx)(n.p,{children:"With Plug-N-Meet, you can. This isn't a complex, multi-month integration project. It's a 15-minute configuration change."}),"\n",(0,s.jsxs)(n.p,{children:["This guide will show you how to add world-class ",(0,s.jsx)(n.strong,{children:"AI meeting assistant features"}),"\u2014powered by ",(0,s.jsx)(n.strong,{children:"Microsoft Azure"})," for translation and ",(0,s.jsx)(n.strong,{children:"Google Gemini"})," for your ",(0,s.jsx)(n.strong,{children:"meeting summary AI"})," and live chat assistant\u2014to your Plug-N-Meet integration."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"the-goal",children:"The Goal"}),"\n",(0,s.jsxs)(n.p,{children:["By the end of this guide, your ",(0,s.jsx)(n.strong,{children:"meeting assistant AI"})," will be able to:"]}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Provide live, real-time captions and translation."}),"\n",(0,s.jsx)(n.li,{children:"Answer questions and provide help through a dedicated, private chat channel."}),"\n",(0,s.jsxs)(n.li,{children:["Generate a full ",(0,s.jsx)(n.strong,{children:"meeting summary AI"})," with key decisions and action items after the session ends."]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["A running Plug-N-Meet server. If you don't have one, follow our ",(0,s.jsx)(n.a,{href:"/docs/installation",children:"Installation Guide"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:["API keys from your chosen AI providers:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"For Live Translation:"})," An API Key and Region from ",(0,s.jsx)(n.a,{href:"https://azure.microsoft.com/en-us/products/ai-services/speech-to-text",children:"Microsoft Azure's Cognitive Services"}),"."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"For AI Chat & Summaries:"})," An API Key from ",(0,s.jsx)(n.a,{href:"https://aistudio.google.com/app/apikey",children:"Google AI Studio"})," for the Gemini API."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"step-1-configure-the-ai-providers",children:"Step 1: Configure the AI Providers"}),"\n",(0,s.jsxs)(n.p,{children:["The first step is to tell your Plug-N-Meet server how to connect to the AI services. Open your ",(0,s.jsx)(n.code,{children:"config.yaml"})," file on your server and find the ",(0,s.jsx)(n.code,{children:"insights"})," section."]}),"\n",(0,s.jsxs)(n.p,{children:["The new configuration is highly flexible. First, you define your ",(0,s.jsx)(n.code,{children:"providers"})," (your accounts), and then you assign those providers to specific ",(0,s.jsx)(n.code,{children:"services"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Here is a minimal configuration to enable all three features:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'insights:\n  enabled: true\n  # 1. Define all available provider accounts ONCE.\n  providers:\n    azure:\n      - id: "my-azure-account" # A unique name you choose\n        credentials:\n          api_key: "YOUR_AZURE_KEY"\n          region: "eastus"\n    google:\n      - id: "my-gemini-account" # A unique name you choose\n        credentials:\n          api_key: "YOUR_GEMINI_API_KEY"\n\n  # 2. Define the services that USE the providers.\n  services:\n    # Transcription is required for both live captions and translation.\n    transcription:\n      provider: "azure"\n      id: "my-azure-account"\n\n    # The AI text chat assistant.\n    ai_text_chat:\n      provider: "google"\n      id: "my-gemini-account"\n      options:\n        chat_model: "gemini-2.5-pro" # Powerful model for in-depth answers\n\n    # The meeting summarizer will use the audio from the transcription service.\n    meeting_summarizing:\n      provider: "google"\n      id: "my-gemini-account"\n      options:\n        # Use gemini-2.5-flash for faster, cost-effective summaries.\n        summarize_model: "gemini-2.5-flash"\n'})}),"\n",(0,s.jsxs)(n.p,{children:["Save the file and ",(0,s.jsx)(n.strong,{children:"restart your PlugNmeet server"})," for the changes to take effect."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"sudo systemctl restart plugnmeet\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"step-2-enable-ai-features-in-your-room",children:"Step 2: Enable AI Features in Your Room"}),"\n",(0,s.jsxs)(n.p,{children:["Now that the server is configured, you can enable these features on a per-room basis. When you make your ",(0,s.jsx)(n.code,{children:"createRoom"})," API call, add the ",(0,s.jsx)(n.code,{children:"insights_features"})," block to your metadata."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'{\n  "room_id": "ai-powered-room",\n  "metadata": {\n    "room_features": {\n      // ... other features like allow_webcams, etc.\n      "insights_features": {\n        "is_allow": true,\n        "transcription_features": {\n          "is_allow": true,\n          "is_allow_translation": true\n        },\n        "ai_features": {\n          "is_allow": true,\n          "ai_text_chat_features": {\n            "is_allow": true\n          },\n          "meeting_summarization_features": {\n            "is_allow": true\n          }\n        }\n      }\n    }\n  }\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"That's it! Any room created with this metadata will now have the AI Meeting Assistant enabled."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"step-3-experience-it-live",children:"Step 3: Experience It Live"}),"\n",(0,s.jsx)(n.p,{children:"When a user joins a room created with these settings, the AI features are available to be activated."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Live Captions & Translation:"})," A moderator must first enable the service from the ",(0,s.jsx)(n.strong,{children:"3-dots menu > Transcription and Translation"}),". Once enabled, participants will see a new ",(0,s.jsx)(n.strong,{children:'"T" icon'})," in their main control bar. Clicking this opens a menu where they can view the live captions and select their own preferred language for translation."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Interactive AI Chat Assistant:"})," Any user can interact with the AI assistant. In the side panel, a new ",(0,s.jsx)(n.strong,{children:'"AI Assistant"'})," tab will appear. Clicking this opens a dedicated chat interface where a user can privately ask the AI questions about the meeting content, get clarifications, or ask for help without cluttering the main participant chat."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Automated Meeting Notes:"})," To generate ",(0,s.jsx)(n.strong,{children:"AI meeting notes"}),", a moderator must start the service from the ",(0,s.jsx)(n.strong,{children:"3-dots menu > AI Tools > Meeting Summarization"}),". The ",(0,s.jsx)(n.strong,{children:"meeting assistant AI"})," will then process the audio in the background."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,s.jsxs)(n.p,{children:["In just a few minutes, you've transformed a standard video call into an intelligent, interactive, and globally inclusive meeting experience. By leveraging Plug-N-Meet's provider-agnostic AI layer, you can easily add a powerful ",(0,s.jsx)(n.strong,{children:"meeting assistant AI"})," that gives you a significant edge, all while maintaining full control over your self-hosted platform."]}),"\n",(0,s.jsxs)(n.p,{children:["The real power of this platform is that your data isn't trapped. After the meeting ends, you can use the ",(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.a,{href:"/docs/api/artifact/fetch",children:"Artifacts API"})})," to programmatically retrieve the summary and transcription, allowing you to build powerful integrations with your new ",(0,s.jsx)(n.strong,{children:"AI meeting notes"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},31894:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>o});var i=t(59471);const s={},a=i.createContext(s);function r(e){const n=i.useContext(a);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:r(e.components),i.createElement(a.Provider,{value:n},e.children)}},43502:e=>{e.exports=JSON.parse('{"permalink":"/blog/how-to-add-ai-meeting-assistant-features","editUrl":"https://github.com/mynaparrot/plugNmeet-docs/edit/main/blog/2026-01-03-how-to-add-ai-features.md","source":"@site/blog/2026-01-03-how-to-add-ai-features.md","title":"How to Add an Interactive AI Assistant to Your Video App in 15 Minutes","description":"In today\'s competitive landscape, a basic video call is no longer enough. Users expect intelligent, accessible, and productive experiences. What if you could offer an AI meeting assistant that provides live translation, generates AI meeting notes, and can even answer questions directly in the chat?","date":"2026-01-03T00:00:00.000Z","tags":[{"inline":true,"label":"tutorial","permalink":"/blog/tags/tutorial"},{"inline":true,"label":"how-to","permalink":"/blog/tags/how-to"},{"inline":true,"label":"ai-meeting-assistant","permalink":"/blog/tags/ai-meeting-assistant"},{"inline":true,"label":"ai-text-chat","permalink":"/blog/tags/ai-text-chat"},{"inline":true,"label":"ai-meeting-notes","permalink":"/blog/tags/ai-meeting-notes"},{"inline":true,"label":"meeting-summary-ai","permalink":"/blog/tags/meeting-summary-ai"},{"inline":true,"label":"ai-meeting-summarizer","permalink":"/blog/tags/ai-meeting-summarizer"},{"inline":true,"label":"meeting-assistant-ai","permalink":"/blog/tags/meeting-assistant-ai"},{"inline":true,"label":"developer","permalink":"/blog/tags/developer"}],"readingTime":3.9,"hasTruncateMarker":true,"authors":[{"name":"Jibon L. Costa","title":"Founding developer","url":"https://github.com/jibon57","imageURL":"https://github.com/jibon57.png","key":"jibon","page":null}],"frontMatter":{"title":"How to Add an Interactive AI Assistant to Your Video App in 15 Minutes","slug":"how-to-add-ai-meeting-assistant-features","authors":["jibon"],"tags":["tutorial","how-to","ai-meeting-assistant","ai-text-chat","ai-meeting-notes","meeting-summary-ai","ai-meeting-summarizer","meeting-assistant-ai","developer"]},"unlisted":false,"prevItem":{"title":"How to Enable End-to-End Encryption in Your Video App","permalink":"/blog/how-to-enable-end-to-end-encryption"},"nextItem":{"title":"Stream Like a Pro: How to Bring OBS into Your plugNmeet Room","permalink":"/blog/obs-rtmp-whip-ingress"}}')}}]);